# Atividade 10: Compara√ß√£o Q-Learning vs SARSA (Maze 8x8)

Este projeto implementa e compara dois algoritmos cl√°ssicos de Aprendizado por Refor√ßo (Reinforcement Learning) ‚Äî **Q-Learning** e **SARSA** ‚Äî aplicados √† resolu√ß√£o de um labirinto determin√≠stico 8x8 (Ambiente FrozenLake customizado da Atividade 9).

O objetivo principal √© analisar como cada algoritmo converge para a solu√ß√£o √≥tima em um ambiente sem incerteza (`is_slippery=False`).

##  Descri√ß√£o do Problema

O agente deve navegar de um ponto inicial (S) at√© um objetivo (G) em um grid 8x8, evitando buracos (H) e caminhando sobre o gelo (F).

*   **Ambiente:** FrozenLake-v1 (Gymnasium) com mapa customizado.
*   **Din√¢mica:** Determin√≠stica. O agente sempre vai para a dire√ß√£o escolhida.
*   **Sistema de Recompensas:**
    *   **Objetivo (G):** +1.0
    *   **Buraco (H):** 0.0 (Fim do epis√≥dio)
    *   **Passo no Gelo (F):** -0.01 (Penalidade para incentivar o caminho mais curto).

##  Algoritmos Comparados

### 1. Q-Learning (Off-Policy)
O agente aprende a utilidade de uma a√ß√£o baseando-se na melhor a√ß√£o poss√≠vel no pr√≥ximo estado ($max\_Q(s', a')$), independentemente da a√ß√£o que ele realmente vai tomar.
*   **Caracter√≠stica:** Busca a otimalidade "gananciosa" (greedy). Tende a encontrar o caminho mais curto poss√≠vel, assumindo que n√£o haver√° erros.

### 2. SARSA (On-Policy)
O agente aprende baseando-se na pr√≥xima a√ß√£o real que ele vai tomar ($Q(s', a')$), seguindo sua pol√≠tica atual (que inclui explora√ß√£o aleat√≥ria).
*   **Nome:** State-Action-Reward-State-Action.
*   **Caracter√≠stica:** Leva em conta a sua pr√≥pria pol√≠tica de explora√ß√£o. Em ambientes perigosos (com vento ou gelo escorregadio), o SARSA costuma aprender caminhos mais seguros (longe dos buracos) do que o Q-Learning.

## ‚öôÔ∏è Configura√ß√£o e Hiperpar√¢metros

Os seguintes par√¢metros foram utilizados para garantir a converg√™ncia em 100.000 epis√≥dios:

| Par√¢metro | Valor | Descri√ß√£o |
| :--- | :---: | :--- |
| **Epis√≥dios** | 100.000 | Total de ciclos de treinamento. |
| **Learning Rate ($\alpha$)** | 0.75 | Taxa de aprendizado (velocidade de atualiza√ß√£o da Q-Table). |
| **Gamma ($\gamma$)** | 0.95 | Fator de desconto (import√¢ncia das recompensas futuras). |
| **Epsilon ($\epsilon$)** | 1.0 $\to$ 0.01 | Taxa de explora√ß√£o (cai exponencialmente). |
| **Decay Rate** | 0.00008 | Ajuste fino para manter a explora√ß√£o ativa por tempo suficiente. |

## üìä Resultados Esperados

Ao executar o c√≥digo, dois resultados principais s√£o gerados:

1.  **Gr√°fico de Converg√™ncia:** Mostra a evolu√ß√£o da recompensa m√©dia ao longo do tempo.
    *   *Expectativa:* Em um ambiente determin√≠stico, as curvas do Q-Learning e do SARSA devem ser muito parecidas, ambas subindo e estabilizando pr√≥ximas √† recompensa m√°xima.
2.  **Avalia√ß√£o (Teste Cego):** Ap√≥s o treino, os agentes s√£o testados por 100 epis√≥dios sem explora√ß√£o ($\epsilon = 0$).
    *   *Conclus√£o T√≠pica:* Ambos os algoritmos atingem 100% de taxa de sucesso e recompensas id√™nticas, provando que, sem aleatoriedade no ambiente, a pol√≠tica √≥tima √© a mesma para ambos.

## üöÄ Como Executar

Certifique-se de ter as depend√™ncias instaladas:

```bash
pip install gymnasium numpy matplotlib
```

Basta executar o script Python fornecido. O script ir√° treinar ambos os agentes sequencialmente e exibir o gr√°fico comparativo ao final.